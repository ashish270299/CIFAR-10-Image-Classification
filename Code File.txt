# -*- coding: utf-8 -*-
"""DL_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W0A7QtnqhfjOcXwBZVNvuAGuuZmFV8B-
"""

!pip install scikit-plot

"""##Importing Libraries"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import os
import keras

from keras import layers
from keras import models
from keras import optimizers, regularizers
from keras.optimizers import SGD
from keras.datasets import cifar10
from keras.models import Sequential
from keras.models import load_model
from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization
from keras.layers import Conv2D, MaxPooling2D, MaxPool2D
from keras.models import model_from_json
from keras.callbacks import CSVLogger
from keras.preprocessing.image import ImageDataGenerator
from keras.utils import plot_model
from collections import Counter

from sklearn.metrics import roc_curve
from sklearn.metrics import auc
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import normalize
import scikitplot as skplt

"""##Loading the Dataset"""

(X_train, X_test), (y_train, y_test) = cifar10.load_data()

print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

X_train

"""## Data Analysis"""

# Visualize
fig, axes = plt.subplots(3, 3)
fig.subplots_adjust(hspace=0.6, wspace=0.3)

class_to_label =  ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog',
                   'Frog', 'Horse', 'Ship', 'Truck']
print(class_to_label)
for i, ax in enumerate(axes.flat):
    # Plot image.
    ax.imshow(X_train[i])

    # Name of the true class.
    cls_true_name = class_to_label[X_test[i][0]]
    xlabel = "class: {0}".format(cls_true_name)

    # Show the classes as the label on the x-axis.
    ax.set_xlabel(xlabel)

    # Remove ticks from the plot.
    ax.set_xticks([])
    
num_classes = np.unique(X_test).shape[0]
print("Number of training examples:", X_train.shape[0])
print("Number of testing examples:", y_train.shape[0])
print("Number of classes:", num_classes)
print("Image shape:", X_train[0].shape)
print("Image data type:", X_train.dtype)
    
reshaped_labels = [class_num[0] for class_num in X_test]
class_distribution = Counter(reshaped_labels)
x = range(10)
y = [class_distribution[cls] for cls in x]
plt.figure(figsize=(12,8))
plt.xticks(x)
plt.title("Number of training examples in each class")
plt.xlabel("Class")
plt.ylabel("Number of examples")
plt.bar(x, y)

reshaped_labels = [class_num[0] for class_num in y_test]
class_distribution = Counter(reshaped_labels)
x1 = range(10)
y1 = [class_distribution[cls] for cls in x]
plt.figure(figsize=(12,8))
plt.xticks(x)
plt.title("Number of testing examples in each class")
plt.xlabel("Class")
plt.ylabel("Number of examples")
plt.bar(x1, y1)

# The class are equally distributed

"""## Preprocessing the Data

Further, we move on to one hot encoding as the classes are in the character data type and we need it to convert into integer data type for applying it into the models. With the help of this encoding, the number may be converted into a binary vector with 10 elements and an index for the class value of 1. The to_categorical() utility function allows us to accomplish this.
"""

num_classes = 10
# Convert class vectors to binary class matrices.
X_test = keras.utils.to_categorical(X_test, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

"""Then we perform data augmentation which makes copies of examples in the dataset with small modifications. This has a regularizing effect which can help in performance and we have various augmentations to perform such as rotating the image, shifting the width or height of the image, or flipping the image."""

# Data agumentation
datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    )
datagen.fit(X_train)

"""## Normalizing the Data

It is known that each image has been assigned to a value from range 0 to 255, as we are not aware of the best way to scale the values, but we know scaling is needed, so to achieve that the data or values are normalized in the range from 0 to 1. This approach is done by first converting the integer value to float value and then dividing it by 255. Now the data is ready to be implemented on different models.
"""

# Normalization
X_train = X_train.astype('float32')
y_train = y_train.astype('float32')
X_train /= 255
y_train /= 255


     

# Train weights
def fit_save_model(model, epochs, name=None):
    ''' Fit and save a model, or load from disk '''
    print()
    model.summary()
    if not os.path.isfile('{}.csv'.format(str(name))) or not os.path.isfile('{}.h5'.format(str(name))):
        csv_logger = CSVLogger('{}.csv'.format(str(name)), 
                               separator=',', 
                               append=False)
        model.fit(X_train, X_test,
                  batch_size=128,
                  epochs=epochs,
                  validation_split = 0.25,
                  callbacks = [csv_logger])
        # serialize weights to HDF5
        model.save_weights('{}.h5'.format(str(name)))
        print('Saved model to disk')
        history = model.history.history
    else:
        log_data = pd.read_csv('{}.csv'.format(str(name)), 
                               sep=',', 
                               engine='python')
        model.load_weights('{}.h5'.format(str(name)))
        print('Loaded model from disk')
        history = log_data

    test_loss, test_acc = model.evaluate(y_train, y_test)
    print('test_acc:', test_acc)

    return model, history

X_train

"""## Modeling

## 1. Applying Simple Model without Convolutional Layer
"""

model = models.Sequential()

model.add(layers.Flatten(input_shape=X_train.shape[1:]))
model.add(layers.Dense(256, activation='relu'))
model.add(layers.Dropout(0.25))
model.add(layers.Dense(256, activation='relu'))
model.add(layers.Dropout(0.25))
model.add(layers.Dense(256, activation='relu'))
model.add(layers.Dropout(0.25))
model.add(layers.Dense(10, activation='softmax'))

model.compile(optimizer='adam',
                loss='categorical_crossentropy',
                metrics=['accuracy'])

epochs = 50
model, history = fit_save_model(model, epochs, 'mnist_old3')

score = model.evaluate(y_train, y_test, verbose=0)
print("%s: %.2f%%" % (model.metrics_names[1], score[1]*100))

final_loss, final_accuracy = model.evaluate(y_train, y_test)
print('The final loss on the test set is:', final_loss)
print('The final accuracy on the test set is:', final_accuracy)

# plot final losses
plt.figure(figsize=(12, 5))
plt.plot(history['val_loss'], label='Validation loss')
plt.plot(history['loss'], label='Training loss')
plt.plot([epochs-1], [final_loss], 'o', label='Final test loss')
plt.legend()
plt.show()

def report(model, epochs):
    ''' Visualizes precision-recall in as a heatmap and showing the ROC-curve
    ''' 
    #test_score = history.decision_function(y_train)
    pred = model.predict(y_train, batch_size=128, verbose=1)
    predicted = np.argmax(pred, axis=1)
    report = classification_report(np.argmax(y_test, axis=1),
            predicted)
    probas = model.predict(y_train)
    print(report)

    # Compute ROC curve and ROC area for each class
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(10):
        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], probas[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])


    # Compute micro-average ROC curve and ROC area
    fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(),
            probas.ravel())
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
    plt.figure(figsize=(12, 8))
    lw = 2
    plt.plot(fpr[2], tpr[2], color='darkorange',
             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])
    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic curve')
    plt.legend(loc="lower right")
    plt.show()

report(model,epochs)

"""## 2. Simple Model with convolutional layers"""

model = Sequential()
model.add(Conv2D(32, (3, 3), padding='same',
                 input_shape=X_train.shape[1:]))
model.add(Activation('relu'))
x = Conv2D(32, (3, 3))
model.add(x)
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(64, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(num_classes))
model.add(Activation('softmax'))
model.summary()

# initiate RMSprop optimizer
opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)
model.compile(loss='categorical_crossentropy',
                  optimizer=opt,
                  metrics=['accuracy'])
epochs = 100
model, history = fit_save_model(model, epochs, '3')

score = model.evaluate(y_train, y_test, verbose=0)
print("%s: %.2f%%" % (model.metrics_names[1], score[1]*100))

final_loss, final_accuracy = model.evaluate(y_train, y_test)
print('The final loss on the test set is:', final_loss)
print('The final accuracy on the test set is:', final_accuracy)

# plot final losses
plt.figure(figsize=(12, 5))
plt.plot(history['val_loss'], label='Validation loss')
plt.plot(history['loss'], label='Training loss')
plt.plot([epochs-1], [final_loss], 'o', label='Final test loss')
plt.legend()
plt.show()

def report(model, epochs):
    ''' Visualizes precision-recall in as a heatmap and showing the ROC-curve
    ''' 
    #test_score = history.decision_function(y_train)
    pred = model.predict(y_train, batch_size=128, verbose=1)
    predicted = np.argmax(pred, axis=1)
    report = classification_report(np.argmax(y_test, axis=1),
            predicted)
    probas = model.predict(y_train)
    print(report)

    # Compute ROC curve and ROC area for each class
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(10):
        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], probas[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])


    # Compute micro-average ROC curve and ROC area
    fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(),
            probas.ravel())
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
    plt.figure(figsize=(12, 8))
    lw = 2
    plt.plot(fpr[2], tpr[2], color='darkorange',
             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])
    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic curve')
    plt.legend(loc="lower right")
    plt.show()

report(model,epochs)

"""## 3. Complex Model with Batch Normalization, Kernel Initializer"""

model = Sequential()
 model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
 model.add(BatchNormalization())
 model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
 model.add(BatchNormalization())
 model.add(MaxPooling2D((2, 2)))
 model.add(Dropout(0.2))
 model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
 model.add(BatchNormalization())
 model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
 model.add(BatchNormalization())
 model.add(MaxPooling2D((2, 2)))
 model.add(Dropout(0.3))
 model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
 model.add(BatchNormalization())
 model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
 model.add(BatchNormalization())
 model.add(MaxPooling2D((2, 2)))
 model.add(Dropout(0.4))
 model.add(Flatten())
 model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
 model.add(BatchNormalization())
 model.add(Dropout(0.5))
 model.add(Dense(10, activation='softmax'))

# compile model
opt = SGD(lr=0.001, momentum=0.9)
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
epochs = 100
model, history = fit_save_model(model, epochs, 'nn_1')

score = model.evaluate(y_train, y_test, verbose=0)
print("%s: %.2f%%" % (model.metrics_names[1], score[1]*100))

final_loss, final_accuracy = model.evaluate(y_train, y_test)
print('The final loss on the test set is:', final_loss)
print('The final accuracy on the test set is:', final_accuracy)

# plot final losses
plt.figure(figsize=(12, 5))
plt.plot(history['val_loss'], label='Validation loss')
plt.plot(history['loss'], label='Training loss')
plt.plot([epochs-1], [final_loss], 'o', label='Final test loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()
plt.show()

def report(model, epochs):
    ''' Visualizes precision-recall in as a heatmap and showing the ROC-curve
    ''' 
    #test_score = history.decision_function(y_train)
    pred = model.predict(y_train, batch_size=128, verbose=1)
    predicted = np.argmax(pred, axis=1)
    report = classification_report(np.argmax(y_test, axis=1),
            predicted)
    probas = model.predict(y_train)
    print(report)

    # Compute ROC curve and ROC area for each class
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(10):
        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], probas[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])


    # Compute micro-average ROC curve and ROC area
    fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(),
            probas.ravel())
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
    plt.figure(figsize=(12, 8))
    lw = 2
    plt.plot(fpr[2], tpr[2], color='darkorange',
             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])
    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic curve')
    plt.legend(loc="lower right")
    plt.show()

report(model, epochs)

"""## 4. Complex CNN model with batch normailzation, data augmentation, L2 Norm regularization """

#Best performing model
weight_decay = 1e-4
model = Sequential()
model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=X_train.shape[1:]))
model.add(Activation('elu'))
model.add(BatchNormalization())
model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('elu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.2))
 
model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('elu'))
model.add(BatchNormalization())
model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('elu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.3))
 
model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('elu'))
model.add(BatchNormalization())
model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('elu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.4))
 
model.add(Flatten())
model.add(Dense(num_classes, activation='softmax'))

#compile
model.compile(loss='categorical_crossentropy',
                  optimizer=opt,
                  metrics=['accuracy'])

epochs = 150
model, history = fit_save_model(model, epochs, "best1")

''' Visulizing losses and acurracy for a model '''
# plotting losses
plt.figure(figsize=(12, 5))
plt.plot(history['val_loss'], label='Validation loss')
plt.plot(history['loss'], label='Training loss')
plt.legend()
plt.xlabel('epochs')
plt.ylabel('loss')
plt.show()

# plotting accuracy
plt.figure(figsize=(12, 5))
plt.plot(history['val_accuracy'], label='Validation accuracy')
plt.plot(history['accuracy'], label='Training accuracy')
plt.legend()
plt.xlabel('epochs')
plt.ylabel('loss')
plt.show()

final_loss, final_accuracy = model.evaluate(y_train, y_test)
print('The final loss on the test set is:', final_loss)
print('The final accuracy on the test set is:', final_accuracy)

# plotting final losses
plt.figure(figsize=(12, 5))
plt.plot(history['val_loss'], label='Validation loss')
plt.plot(history['loss'], label='Training loss')
plt.plot([epochs-1], [final_loss], 'o', label='Final test loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()
plt.show()

report(model, epochs)